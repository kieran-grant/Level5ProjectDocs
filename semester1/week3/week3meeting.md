# Meeting 03/10/2022

## Minutes
- Went through slides and discussed progress in the past week.
- A bit of focus on the black-box DDSP paper and how it is implemented.
- Narrowed down project scope to the 'DALL-E for sound' idea. Either for audio effects or generating waveforms.
  - Ideas around how UI's can be generated by the user prompts.
  - E.g. if you generate a reverb, and you want it to be warmer - a new control appears with a 'warm' knob which can add/remove warmth.

## To-do for this week 
- Understand method of Black-Box DDSP paper (be able to explain how the gradient estimation works).
- Is there enough NLP basis to start with? Can possibly use word2vec as a backup.
  - Can something like GPT-3 be used instead?  
- CLIP used for DALL-E, need to see how it works and if it can be applied to audio.
  - Could potentially apply directly to spectrograms?
- Think about the audio/example pairs required for the task - audio effects dataset?
- Think about how we can use the black-box DDSP model to optimise sound.
  - Can we use it for generic plugins? 
  - I.e. you already have a reverb plugin, but you want it warmer - it adjusts the parameters to increase the warmth?
- Are there any examples of audio engineers/producers discussing sounds with examples?
- Where does this non-technical descriptive language of timbre come from? 
  - Are there any studies about language used in music for descriptions of timbre? 